from flask import Flask, render_template, request, jsonify, abort, send_from_directory
import os
import json
import sqlite3
import requests
import anthropic
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from datetime import datetime, timedelta, UTC
import logging
from logging.handlers import RotatingFileHandler
from werkzeug.middleware.proxy_fix import ProxyFix
import re
import plotly.graph_objects as go
from stop_words import get_stop_words
from newspaper import Article
import html

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è Flask
app = Flask(__name__, static_folder='static', template_folder='templates')
app.wsgi_app = ProxyFix(app.wsgi_app, x_for=1, x_proto=1, x_host=1, x_prefix=1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
    file_handler = RotatingFileHandler('app.log', maxBytes=1024*1024, backupCount=5)
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    app.logger.addHandler(file_handler)
    app.logger.addHandler(console_handler)
    app.logger.setLevel(logging.INFO)

    class RequestFilter(logging.Filter):
        def filter(self, record):
            return not (hasattr(record, 'msg') and '404 Not Found' in record.msg)

    logging.getLogger('werkzeug').addFilter(RequestFilter())
    logging.getLogger('werkzeug').setLevel(logging.WARNING)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()
ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
NEWS_API_KEY = os.getenv('NEWS_API_KEY')
NEWS_API_ENABLED = bool(NEWS_API_KEY)
MODEL_NAME = "claude-3-opus-20240229"

if not ANTHROPIC_API_KEY:
    raise ValueError("ANTHROPIC_API_KEY is missing! Please set it in your .env file.")
if not NEWS_API_KEY:
    app.logger.warning("NEWS_API_KEY is missing! Similar news functionality will be disabled.")
    NEWS_API_ENABLED = False

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
DB_NAME = 'news_analysis.db'

# –ù–∞—á–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
INITIAL_SOURCE_COUNTS = {
    "bbc.com": {"high": 15, "medium": 5, "low": 1},
    "reuters.com": {"high": 20, "medium": 3, "low": 0},
    "foxnews.com": {"high": 3, "medium": 7, "low": 15},
    "cnn.com": {"high": 5, "medium": 10, "low": 5},
    "nytimes.com": {"high": 10, "medium": 5, "low": 2},
    "theguardian.com": {"high": 12, "medium": 4, "low": 1},
    "apnews.com": {"high": 18, "medium": 2, "low": 0}
}

# –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–æ–º–µ–Ω–æ–≤ –∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ –°–ú–ò
media_owners = {
    "bbc.com": "BBC",
    "reuters.com": "Thomson Reuters",
    "foxnews.com": "Fox Corporation",
    "cnn.com": "Warner Bros. Discovery",
    "nytimes.com": "The New York Times Company",
    "theguardian.com": "Guardian Media Group",
    "apnews.com": "Associated Press",
    "wsj.com": "News Corp",
    "aljazeera.com": "Al Jazeera Media Network"
}

# ID –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ NewsAPI
TRUSTED_NEWS_SOURCES_IDS = [
    "bbc-news", "reuters", "associated-press", "the-new-york-times",
    "the-guardian-uk", "the-wall-street-journal", "cnn", "al-jazeera-english"
]

stop_words_en = get_stop_words('en')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
setup_logging()

# Middleware –¥–ª—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ WordPress-—Å–∫–∞–Ω–µ—Ä–æ–≤
@app.before_request
def block_wordpress_scanners():
    wordpress_paths = [
        'wp-admin', 'wp-includes', 'wp-content', 'xmlrpc.php',
        'wp-login.php', 'wp-config.php', 'readme.html', 'license.txt'
    ]
    if any(path in request.path.lower() for path in wordpress_paths):
        app.logger.warning(f"Blocked WordPress scanner request from {request.remote_addr}")
        return abort(404)

# Middleware –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
@app.after_request
def add_security_headers(response):
    response.headers['X-Content-Type-Options'] = 'nosniff'
    response.headers['X-Frame-Options'] = 'SAMEORIGIN'
    response.headers['X-XSS-Protection'] = '1; mode=block'
    response.headers['Referrer-Policy'] = 'strict-origin-when-cross-origin'
    response.headers['Content-Security-Policy'] = "default-src 'self'"
    return response

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ 404 –æ—à–∏–±–æ–∫
@app.errorhandler(404)
def page_not_found(e):
    return render_template('404.html'), 404

# –§—É–Ω–∫—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
def ensure_db_schema():
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ö–µ–º—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
    try:
        conn = sqlite3.connect(DB_NAME)
        c = conn.cursor()
        c.execute('''CREATE TABLE IF NOT EXISTS news (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT,
            source TEXT,
            content TEXT,
            integrity REAL,
            fact_check REAL,
            sentiment REAL,
            bias REAL,
            credibility_level TEXT,
            index_of_credibility REAL,
            url TEXT UNIQUE,
            analysis_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            short_summary TEXT
        )''')
        c.execute('''CREATE TABLE IF NOT EXISTS source_stats (
            source TEXT PRIMARY KEY,
            high INTEGER DEFAULT 0,
            medium INTEGER DEFAULT 0,
            low INTEGER DEFAULT 0,
            total_analyzed INTEGER DEFAULT 0
        )''')
        conn.commit()
        app.logger.info("Database schema ensured successfully")
    except sqlite3.Error as e:
        app.logger.error(f"Error ensuring database schema: {str(e)}")
        raise
    finally:
        if conn:
            conn.close()

def initialize_sources(initial_counts):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    try:
        conn = sqlite3.connect(DB_NAME)
        c = conn.cursor()
        for source, counts in initial_counts.items():
            c.execute("SELECT total_analyzed FROM source_stats WHERE source = ?", (source,))
            row = c.fetchone()
            if row is None:
                high = counts.get("high", 0)
                medium = counts.get("medium", 0)
                low = counts.get("low", 0)
                c.execute('''INSERT INTO source_stats (source, high, medium, low, total_analyzed)
                         VALUES (?, ?, ?, ?, ?)''',
                         (source, high, medium, low, high+medium+low))
        conn.commit()
        app.logger.info("Initial source initialization completed successfully")
    except sqlite3.Error as e:
        app.logger.error(f"Database error during source initialization: {str(e)}")
        raise
    finally:
        if conn:
            conn.close()

def check_database_integrity():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
    try:
        if not os.path.exists(DB_NAME):
            app.logger.error(f"Database file {DB_NAME} not found!")
            return False

        conn = sqlite3.connect(DB_NAME)
        c = conn.cursor()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∞–±–ª–∏—Ü
        c.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [table[0] for table in c.fetchall()]
        required_tables = ['news', 'source_stats']
        for table in required_tables:
            if table not in tables:
                app.logger.error(f"Critical table '{table}' is missing!")
                return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü
        for table, columns in [('news', ['id', 'title', 'source', 'content', 'integrity',
                                      'fact_check', 'sentiment', 'bias', 'credibility_level',
                                      'index_of_credibility', 'url', 'analysis_date', 'short_summary']),
                             ('source_stats', ['source', 'high', 'medium', 'low', 'total_analyzed'])]:
            c.execute(f"PRAGMA table_info({table})")
            columns_in_table = [row[1] for row in c.fetchall()]
            for column in columns:
                if column not in columns_in_table:
                    app.logger.error(f"Critical column '{column}' is missing in '{table}' table!")
                    return False

        return True
    except Exception as e:
        app.logger.error(f"Error during database check: {str(e)}")
        return False
    finally:
        if conn:
            conn.close()

# –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
class ClaudeNewsAnalyzer:
    def __init__(self, api_key, model_name):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.model_name = model_name

    def analyze_article_text(self, article_text_content, source_name_for_context):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º API Claude."""
        max_chars_for_claude = 10000
        if len(article_text_content) > max_chars_for_claude:
            article_text_content = article_text_content[:max_chars_for_claude]

        media_owner_display = media_owners.get(source_name_for_context, "Unknown Owner")

        prompt = f"""–í—ã - –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é –∏ –≤–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.

–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏:
\"\"\"
{article_text_content}
\"\"\"

–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name_for_context}
–í–ª–∞–¥–µ–ª–µ—Ü –°–ú–ò: {media_owner_display}

–í–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º–∏ –ø–æ–ª—è–º–∏:
1. news_integrity (0.0-1.0) - —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
2. fact_check_needed_score (0.0-1.0) - –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤
3. sentiment_score (0.0-1.0) - —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω
4. bias_score (0.0-1.0) - —Å—Ç–µ–ø–µ–Ω—å –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏
5. index_of_credibility (0.0-1.0) - –æ–±—â–∏–π –∏–Ω–¥–µ–∫—Å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
6. short_summary - –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏
7. topics - –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã —Å—Ç–∞—Ç—å–∏
8. key_arguments - –∫–ª—é—á–µ–≤—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∞–≤—Ç–æ—Ä–∞
9. mentioned_facts - —É–ø–æ–º—è–Ω—É—Ç—ã–µ —Ñ–∞–∫—Ç—ã
10. potential_biases_identified - –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏"""

        try:
            message = self.client.messages.create(
                model=self.model_name,
                max_tokens=2000,
                messages=[{"role": "user", "content": prompt}]
            )
            return json.loads(message.content[0].text)
        except Exception as e:
            app.logger.error(f"Error during Claude analysis: {str(e)}")
            raise

def extract_text_from_url(url):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ URL."""
    try:
        clean_url = re.sub(r'/amp(/)?$', '', url)
        article = Article(clean_url)
        article.download()
        article.parse()
        text = article.text.strip()
        title = article.title.strip() if article.title else ""
        source = urlparse(clean_url).netloc.replace("www.", "")
        return text, source, title
    except Exception as e:
        app.logger.error(f"Error extracting article from URL {url}: {str(e)}")
        return "", "", ""

def calculate_credibility_level(analysis_result):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞."""
    integrity = analysis_result.get('news_integrity', 0.0)
    fact_check = analysis_result.get('fact_check_needed_score', 1.0)
    sentiment = analysis_result.get('sentiment_score', 0.5)
    bias = analysis_result.get('bias_score', 1.0)

    # –†–∞—Å—á–µ—Ç –∏–Ω–¥–µ–∫—Å–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
    index = (integrity * 0.45) + ((1.0 - fact_check) * 0.35) + ((1.0 - abs(sentiment - 0.5) * 2) * 0.10) + ((1.0 - bias) * 0.10)

    if index >= 0.75:
        return 'High', index
    elif index >= 0.5:
        return 'Medium', index
    else:
        return 'Low', index

def save_analysis_to_db(url, title, source, content, analysis_result):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
    try:
        conn = sqlite3.connect(DB_NAME)
        c = conn.cursor()

        # –†–∞—Å—á–µ—Ç —É—Ä–æ–≤–Ω—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
        credibility_level, index_of_credibility = calculate_credibility_level(analysis_result)

        c.execute('''INSERT INTO news (url, title, source, content, integrity, fact_check, sentiment, bias,
                     credibility_level, index_of_credibility, short_summary)
                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                (url, title, source, content,
                 analysis_result.get('news_integrity', 0.0),
                 analysis_result.get('fact_check_needed_score', 1.0),
                 analysis_result.get('sentiment_score', 0.5),
                 analysis_result.get('bias_score', 1.0),
                 credibility_level, index_of_credibility,
                 analysis_result.get('short_summary', 'No summary')))

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        c.execute("SELECT high, medium, low FROM source_stats WHERE source = ?", (source,))
        row = c.fetchone()
        if row:
            high, medium, low = row
            if credibility_level == 'High': high += 1
            elif credibility_level == 'Medium': medium += 1
            else: low += 1
            c.execute('''UPDATE source_stats SET high=?, medium=?, low=? WHERE source=?''',
                    (high, medium, low, source))
        else:
            high = 1 if credibility_level == 'High' else 0
            medium = 1 if credibility_level == 'Medium' else 0
            low = 1 if credibility_level == 'Low' else 0
            c.execute('''INSERT INTO source_stats (source, high, medium, low, total_analyzed)
                        VALUES (?, ?, ?, ?, 1)''', (source, high, medium, low))

        conn.commit()
        return credibility_level, index_of_credibility
    except Exception as e:
        app.logger.error(f"Error saving analysis to database: {str(e)}")
        raise
    finally:
        if conn:
            conn.close()

def process_article_analysis(input_text, source_name_manual):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏."""
    if input_text.strip().startswith("http"):
        article_url = input_text.strip()
        content, source, title = extract_text_from_url(article_url)
        if not content:
            return "–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞", None, None
    else:
        content = input_text
        source = source_name_manual if source_name_manual else "Direct Input"
        title = "User-provided Text"

    if len(content) < 100:
        return "–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç", None, None

    analyzer = ClaudeNewsAnalyzer(ANTHROPIC_API_KEY, MODEL_NAME)
    try:
        analysis_result = analyzer.analyze_article_text(content, source)
        credibility_level, index_of_credibility = save_analysis_to_db(
            input_text if input_text.startswith("http") else None,
            title, source, content, analysis_result
        )

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        output_md = f"""### –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏: {title}
–ò—Å—Ç–æ—á–Ω–∏–∫: {source}
–£—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏: {credibility_level} ({index_of_credibility:.2f})

–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:
- –¶–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {analysis_result.get('news_integrity', 0.0):.2f}
- –ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤: {1 - analysis_result.get('fact_check_needed_score', 1.0):.2f}
- –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω: {analysis_result.get('sentiment_score', 0.5):.2f}
- –ü—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å: {1 - analysis_result.get('bias_score', 1.0):.2f}
- –ò–Ω–¥–µ–∫—Å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏: {index_of_credibility:.2f}

–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:
{analysis_result.get('short_summary', '–ù–µ—Ç –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è')}

–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã:
{', '.join(analysis_result.get('topics', []))}"""

        scores_for_chart = {
            "Integrity": analysis_result.get('news_integrity', 0.0) * 100,
            "Factuality": (1 - analysis_result.get('fact_check_needed_score', 1.0)) * 100,
            "Sentiment": analysis_result.get('sentiment_score', 0.5) * 100,
            "Bias": (1 - analysis_result.get('bias_score', 1.0)) * 100,
            "Credibility": index_of_credibility * 100
        }

        return output_md, scores_for_chart, analysis_result
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}", None, None

def generate_query(analysis_result):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π."""
    topics = analysis_result.get('topics', [])
    key_arguments = analysis_result.get('key_arguments', [])
    mentioned_facts = analysis_result.get('mentioned_facts', [])

    all_terms = []
    for phrase_list in [topics, key_arguments]:
        for phrase in phrase_list:
            if not phrase.strip(): continue
            if ' ' in phrase.strip() and len(phrase.strip().split()) > 1:
                all_terms.append(f'"{phrase.strip()}"')
            else:
                all_terms.append(phrase.strip())

    for fact in mentioned_facts:
        if not fact.strip(): continue
        words = [word for word in fact.lower().split() if word not in stop_words_en and len(word) > 2]
        all_terms.extend(words)

    unique_terms = list(set(all_terms))

    if len(unique_terms) >= 3:
        query = " AND ".join(unique_terms)
    elif unique_terms:
        query = " OR ".join(unique_terms)
    else:
        query = "current events OR news"

    return query

def fetch_similar_news(analysis_result, days_range=7, max_articles=3):
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å –≤—ã—Å–æ–∫–∏–º –∏–Ω–¥–µ–∫—Å–æ–º –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏."""
    if not NEWS_API_ENABLED:
        return []

    initial_query = generate_query(analysis_result)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
    original_published_date_str = analysis_result.get('published_date', 'N/A')
    end_date = datetime.now(UTC).date()

    if original_published_date_str and original_published_date_str != 'N/A':
        try:
            parsed_date = datetime.strptime(original_published_date_str, '%Y-%m-%d').date()
            start_date = parsed_date - timedelta(days=days_range)
            end_date = parsed_date + timedelta(days=days_range)
        except ValueError:
            start_date = end_date - timedelta(days=days_range)
    else:
        start_date = end_date - timedelta(days=days_range)

    # –ü–æ–ø—ã—Ç–∫–∞ 1: –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å —Å –Ω–∞–¥–µ–∂–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
    params_specific = {
        "q": initial_query,
        "apiKey": NEWS_API_KEY,
        "language": "en",
        "pageSize": max_articles * 2,
        "sortBy": "relevancy",
        "from": start_date.strftime('%Y-%m-%d'),
        "to": end_date.strftime('%Y-%m-%d'),
    }

    if TRUSTED_NEWS_SOURCES_IDS:
        params_specific["sources"] = ",".join(TRUSTED_NEWS_SOURCES_IDS)

    articles_found = []
    try:
        response = requests.get("https://newsapi.org/v2/everything", params=params_specific, timeout=15)
        response.raise_for_status()
        articles_found = response.json().get("articles", [])
    except Exception as e:
        app.logger.error(f"Error fetching similar news: {str(e)}")

    # –ü–æ–ø—ã—Ç–∫–∞ 2: –ë–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π –∑–∞–ø—Ä–æ—Å, –µ—Å–ª–∏ –ø–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞ –¥–∞–ª–∞ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if len(articles_found) < max_articles and initial_query != "current events OR news":
        broader_query_terms = list(set(analysis_result.get('topics', [])[:3]))
        broader_query = " OR ".join([f'"{term}"' if ' ' in term else term for term in broader_query_terms if term and term not in stop_words_en])

        if not broader_query:
            broader_query = "current events OR news"

        params_broad = {
            "q": broader_query,
            "apiKey": NEWS_API_KEY,
            "language": "en",
            "pageSize": max_articles * 2,
            "sortBy": "relevancy",
            "from": start_date.strftime('%Y-%m-%d'),
            "to": end_date.strftime('%Y-%m-%d'),
        }

        if TRUSTED_NEWS_SOURCES_IDS:
            params_broad["sources"] = ",".join(TRUSTED_NEWS_SOURCES_IDS)

        try:
            response = requests.get("https://newsapi.org/v2/everything", params=params_broad, timeout=15)
            response.raise_for_status()
            articles_found.extend(response.json().get("articles", []))
        except Exception as e:
            app.logger.error(f"Error fetching similar news: {str(e)}")

    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    unique_articles = {}
    for article in articles_found:
        if article.get('url'):
            unique_articles[article['url']] = article
    articles_found = list(unique_articles.values())

    # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–µ–π
    ranked_articles = []
    predefined_trust_scores = {
        "bbc.com": 0.9, "bbc.co.uk": 0.9, "reuters.com": 0.95, "apnews.com": 0.93,
        "nytimes.com": 0.88, "theguardian.com": 0.85, "wsj.com": 0.82,
        "cnn.com": 0.70, "foxnews.com": 0.40, "aljazeera.com": 0.80
    }

    all_query_terms = []
    if 'initial_query' in locals():
        all_query_terms.extend([t.lower().replace('"', '') for t in initial_query.split(' AND ')])
    if 'broader_query' in locals():
        all_query_terms.extend([t.lower().replace('"', '') for t in broader_query.split(' OR ')])
    all_query_terms = list(set([t for t in all_query_terms if t and t not in stop_words_en]))

    for article in articles_found:
        source_domain = urlparse(article.get("url", '')).netloc.replace('www.', '')
        trust_score = predefined_trust_scores.get(source_domain, 0.5)
        article_text = (article.get('title', '') + " " + article.get('description', '')).lower()
        relevance_score = sum(1 for term in all_query_terms if term in article_text)
        final_score = (relevance_score * 10) + (trust_score * 5)
        ranked_articles.append((article, final_score))

    ranked_articles.sort(key=lambda item: item[1], reverse=True)
    return [item[0] for item in ranked_articles[:max_articles]]

def render_similar_articles_html(articles):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç HTML –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π."""
    if not articles:
        return "<p>No similar articles found for the selected criteria.</p>"

    predefined_trust_scores = {
        "bbc.com": 0.9, "bbc.co.uk": 0.9, "reuters.com": 0.95, "apnews.com": 0.93,
        "nytimes.com": 0.88, "theguardian.com": 0.85, "wsj.com": 0.82,
        "cnn.com": 0.70, "foxnews.com": 0.40, "aljazeera.com": 0.80
    }

    try:
        conn = sqlite3.connect(DB_NAME)
        c = conn.cursor()

        html_items = []
        for art in articles:
            title = html.escape(art.get("title", "No Title"))
            article_url = html.escape(art.get("url", "#"))
            source_api_name = html.escape(art.get("source", {}).get("name", "Unknown Source"))

            published_at_raw = art.get('publishedAt', 'N/A')
            published_at_display = html.escape(published_at_raw.split('T')[0] if 'T' in published_at_raw and published_at_raw != 'N/A' else published_at_raw)

            description_raw = art.get('description', 'No description available.')
            if description_raw.startswith(art.get("title", "")):
                description_raw = description_raw[len(art.get("title", "")):].strip()
                if description_raw.startswith("- "):
                    description_raw = description_raw[2:].strip()
            description_display = html.escape(description_raw)

            domain = urlparse(art.get("url", "#")).netloc.replace('www.', '')
            trust_display = ""

            c.execute("SELECT high, medium, low, total_analyzed FROM source_stats WHERE source = ?", (domain,))
            row = c.fetchone()

            if row:
                high, medium, low, total_analyzed = row
                if total_analyzed > 0:
                    score = (high * 1.0 + medium * 0.5 + low * 0.0) / total_analyzed
                    trust_display = f" (Hist. Src. Credibility: {score*100:.0f}%)"

            if not trust_display and domain in predefined_trust_scores:
                predefined_score = predefined_trust_scores.get(domain)
                trust_display = f" (Est. Src. Trust: {predefined_score*100:.0f}%)"
            elif not trust_display:
                trust_display = " (Src. Credibility: N/A)"

            html_items.append(
                f"""
                <div class="similar-article">
                    <h4><a href="{article_url}" target="_blank" rel="noopener noreferrer">
                        {title}
                    </a></h4>
                    <p><strong>Source:</strong> {source_api_name}{trust_display} | <strong>Published:</strong> {published_at_display}</p>
                    <p>{description_display}</p>
                </div>
                <hr>
                """
            )

        return f"""
        <div class="similar-articles-container">
            <h3>üîó Similar News Articles (Ranked by Relevance & Trust):</h3>
            {"".join(html_items)}
        </div>
        """
    except Exception as e:
        app.logger.error(f"Error rendering similar articles: {str(e)}")
        return "<p>Error retrieving similar articles data.</p>"
    finally:
        if conn:
            conn.close()

def get_source_reliability_data():
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤."""
    try:
        conn = sqlite3.connect(DB_NAME)
        c = conn.cursor()

        c.execute("""
            SELECT source, high, medium, low, total_analyzed
            FROM source_stats
            ORDER BY total_analyzed DESC, source ASC
        """)

        data = c.fetchall()
        sources = []
        credibility_indices = []
        high_counts = []
        medium_counts = []
        low_counts = []
        total_analyzed_counts = []

        for source, high, medium, low, total in data:
            total_current = high + medium + low
            if total_current == 0:
                score = 0.5
            else:
                score = (high * 1.0 + medium * 0.5 + low * 0.0) / total_current

            sources.append(source)
            credibility_indices.append(round(score, 2))
            high_counts.append(high)
            medium_counts.append(medium)
            low_counts.append(low)
            total_analyzed_counts.append(total_current)

        return {
            'sources': sources,
            'credibility_indices_for_plot': credibility_indices,
            'high_counts': high_counts,
            'medium_counts': medium_counts,
            'low_counts': low_counts,
            'total_analyzed_counts': total_analyzed_counts
        }
    except Exception as e:
        app.logger.error(f"Error getting source reliability data: {str(e)}")
        return {
            'sources': [],
            'credibility_indices_for_plot': [],
            'high_counts': [],
            'medium_counts': [],
            'low_counts': [],
            'total_analyzed_counts': []
        }
    finally:
        if conn:
            conn.close()

def get_analysis_history_html():
    """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
    try:
        conn = sqlite3.connect(DB_NAME)
        c = conn.cursor()

        c.execute("""
            SELECT url, title, source, credibility_level, short_summary,
                   strftime('%Y-%m-%d %H:%M', analysis_date) as formatted_date
            FROM news
            ORDER BY analysis_date DESC
            LIMIT 15
        """)

        rows = c.fetchall()

        if not rows:
            return "<p>No analysis history yet. Analyze an article to see it appear here!</p>"

        html_items = []
        for url, title, source, credibility, short_summary, date_str in rows:
            display_title = title[:70] + '...' if title and len(title) > 70 else title if title else "N/A"
            source_display = source if source else "N/A"
            link_start = f"<a href='{url}' target='_blank' rel='noopener noreferrer'>" if url and url.startswith(('http://', 'https://')) else ""
            link_end = "</a>" if url and url.startswith(('http://', 'https://')) else ""
            summary_display = short_summary if short_summary else 'No summary available.'

            html_items.append(
                f"""
                <li>
                    <strong>{date_str}</strong>: {link_start}{display_title}{link_end} ({source_display}, {credibility})
                    <br>
                    <em>Summary:</em> {summary_display}
                </li>
                """
            )

        return f"<h3>üìú Recent Analyses:</h3><ul>{''.join(html_items)}</ul>"
    except Exception as e:
        app.logger.error(f"Error getting analysis history: {str(e)}")
        return "<p>Error retrieving analysis history.</p>"
    finally:
        if conn:
            conn.close()

# –ú–∞—Ä—à—Ä—É—Ç—ã Flask
@app.route('/')
def index():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze_route():
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏."""
    try:
        data = request.json
        output_md, scores_for_chart, analysis_result = process_article_analysis(
            data.get('input_text'),
            data.get('source_name_manual')
        )

        if analysis_result is None:
            return jsonify({'error_message': output_md}), 400

        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ö–æ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
        similar_news = fetch_similar_news(analysis_result)

        return jsonify({
            'output_md': output_md,
            'scores_for_chart': scores_for_chart,
            'analysis_result': analysis_result,
            'similar_news': render_similar_articles_html(similar_news)
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/similar_articles', methods=['POST'])
def similar_articles_endpoint():
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Ö–æ–∂–∏–µ —Å—Ç–∞—Ç—å–∏."""
    try:
        data = request.json
        analysis_result = data.get('analysis_result')

        if not analysis_result:
            return jsonify({'similar_html': "<p>No analysis result provided to fetch similar articles.</p>"})

        similar_articles_list = fetch_similar_news(analysis_result)
        return jsonify({
            'similar_html': render_similar_articles_html(similar_articles_list)
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/source_reliability_data')
def source_reliability_data_endpoint():
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤."""
    try:
        data = get_source_reliability_data()

        labeled_sources = []
        if data['sources'] and data['credibility_indices_for_plot']:
            for source, score, total in zip(
                data['sources'],
                data['credibility_indices_for_plot'],
                data['total_analyzed_counts']
            ):
                labeled_sources.append(
                    f"{source}<br>Credibility: {score*100:.0f}%<br>Articles: {total}"
                )

        return jsonify({
            'sources': labeled_sources,
            'credibility_indices_for_plot': data['credibility_indices_for_plot'],
            'high_counts': data['high_counts'],
            'medium_counts': data['medium_counts'],
            'low_counts': data['low_counts'],
            'total_analyzed_counts': data['total_analyzed_counts']
        })
    except Exception as e:
        return jsonify({
            'sources': [],
            'credibility_indices_for_plot': [],
            'high_counts': [],
            'medium_counts': [],
            'low_counts': [],
            'total_analyzed_counts': []
        })

@app.route('/analysis_history_html')
def analysis_history_html_endpoint():
    """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–∞."""
    try:
        history_html = get_analysis_history_html()
        return jsonify({'history_html': history_html})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/check_db_integrity')
def check_db_integrity_endpoint():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
    try:
        result = check_database_integrity()
        if result:
            return jsonify({
                'status': 'success',
                'message': 'Database integrity check passed'
            })
        else:
            return jsonify({
                'status': 'error',
                'message': 'Database integrity check failed'
            }), 500
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'Database check failed: {str(e)}'
        }), 500

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
def initialize_database():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
    ensure_db_schema()
    check_database_integrity()

if __name__ == '__main__':
    initialize_database()
    app.run(host='0.0.0.0', port=5000)
